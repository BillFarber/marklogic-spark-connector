plugins {
    id 'java'
    id 'net.saliman.properties' version '1.5.2'
    id "com.marklogic.ml-gradle" version "4.5.1"
}

group 'org.example'
version '1.0-SNAPSHOT'

repositories {
    mavenCentral()
}

dependencies {
  implementation 'org.apache.spark:spark-sql_2.13:3.3.2'

  implementation "org.apache.spark:spark-launcher_2.13:3.3.2"
  implementation "org.apache.spark:spark-catalyst_2.13:3.3.2"
  implementation "org.apache.spark:spark-streaming_2.13:3.3.2"
  implementation "org.apache.spark:spark-core_2.13:3.3.2"
  implementation "com.marklogic:marklogic-client-api:6.1.0"

  testImplementation 'com.marklogic:marklogic-junit5:1.3.0'
  testImplementation "ch.qos.logback:logback-classic:1.3.5"
  testImplementation "org.slf4j:jcl-over-slf4j:1.7.36"
}

test {
    useJUnitPlatform()
}

task testMarkLogicSparkReader(type: JavaExec) {
  description = "testMarkLogicSparkReader"
  classpath = sourceSets.test.runtimeClasspath
  main = "testMarkLogicSparkReader"
  //jvmArgs = ['--add-exports=java.base/sun.nio.ch=ALL-UNNAMED']
}

// References - https://levelup.gitconnected.com/easy-guide-to-create-a-custom-read-data-source-in-apache-spark-3-194afdc9627a
//              https://github.com/aamargajbhiye/big-data-projects.git
//              https://github.com/mongodb/mongo-spark.git
